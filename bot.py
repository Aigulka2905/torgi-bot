# bot.py
import json
import logging
from aiogram import Bot, Dispatcher, F
from aiogram.types import Message
import chromadb
from chromadb.utils.embedding_functions import DefaultEmbeddingFunction
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from sentence_transformers import SentenceTransformer
import ollama
from dotenv import load_dotenv
import os

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
load_dotenv()
bot = Bot(token=os.getenv("BOT_TOKEN"))
KNOWLEDGE_FILE = "knowledge_base.json"
CHROMA_PATH = "./chroma_db"
COLLECTION_NAME = "fz223_rag"

# === –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ===
logging.basicConfig(level=logging.INFO)

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===
bot = Bot(token=TELEGRAM_TOKEN)
dp = Dispatcher()

# –ó–∞–≥—Ä—É–∑–∫–∞ –∑–Ω–∞–Ω–∏–π
with open(KNOWLEDGE_FILE, "r", encoding="utf-8") as f:
    docs = json.load(f)

# –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î
client = chromadb.PersistentClient(path=CHROMA_PATH)
embedding_func = SentenceTransformerEmbeddingFunction(
    model_name="intfloat/multilingual-e5-large-instruct"
)

try:
    collection = client.get_collection(COLLECTION_NAME)
    logging.info("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
except:
    logging.info("üîÑ –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")
    collection = client.create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_func
    )
    collection.add(
        ids=[str(i) for i in range(len(docs))],
        documents=[d["text"] for d in docs],
        metadatas=[{"source": d["source"]} for d in docs]
    )
    logging.info("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞")

# –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
def retrieve_context(query: str, n=7):
    results = collection.query(query_texts=[query], n_results=n)
    return [
        {"text": doc, "source": meta.get("source", "–Ω–µ —É–∫–∞–∑–∞–Ω")}
        for doc, meta in zip(results["documents"][0], results["metadatas"][0])
    ]

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
def generate_answer(question: str, context_items):
    context_str = "\n".join(
        f"{i+1}. {item['text']} (–ò—Å—Ç–æ—á–Ω–∏–∫: {item['source']})"
        for i, item in enumerate(context_items)
    )

    prompt = f"""–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≠–¢–ü ¬´–¢–æ—Ä–≥–∏ –†–§¬ª –∏ 223-–§–ó.
–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —Ç–æ—á–Ω–æ –∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ—Å–≤–µ–Ω–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –¥–ª—è –æ—Ç–≤–µ—Ç–∞.
–ï—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç ‚Äî —Å–∫–∞–∂–∏: ¬´–ü–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º –¥–∞–Ω–Ω—ã–º: [–∫—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥]¬ª. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π.

–í–æ–ø—Ä–æ—Å: {question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ç–æ–ø-—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã):
{context_str}

–û—Ç–≤–µ—Ç:"""

    response = ollama.chat(
        model="qwen2:7b-instruct",
        messages=[{"role": "user", "content": prompt}],
        options={"temperature": 0.1}
    )
    answer = response["message"]["content"].strip()

    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∏—Ö –Ω–µ –≤–∫–ª—é—á–∏–ª–∞
    if "–ò—Å—Ç–æ—á–Ω–∏–∫:" not in answer and context_items:
        sources = list(set(item["source"] for item in context_items))
        if sources:
            answer += f"\n\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n" + "\n".join(f"- {s}" for s in sources)

    return answer

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
@dp.message(F.text)
async def handle_message(message: Message):
    try:
        user_query = message.text.strip()
        context = retrieve_context(user_query)
        answer = generate_answer(user_query, context)
        await message.answer(answer, parse_mode=None)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞: {e}")
        await message.answer("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

# –ó–∞–ø—É—Å–∫
if __name__ == "__main__":
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –ù–∞–∂–º–∏—Ç–µ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.")
    dp.run_polling(bot)
